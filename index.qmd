---
title: "DSA 554 3.0 Spatio-Temporal Data Analysis"
format:
  revealjs:
    slide-number: true
    show-slide-number: all 
jupyter: python3
---

### Course outline

[Course outline](https://thiyanga-spatiotemporal.netlify.app/posts/welcome/)



# Time series

A time series is a sequence of observations taken sequentially in time.

## Time series data vs Cross sectional data

**Time series data:** a set of observations, along with some information about what times those observations were recorded.

**Cross sectional data:** observations that come from different individuals or groups at a single point in time.



---

## Deterministic vs Non-deterministic time series

**Deterministic time series:** future values can be exactly determined by using some mathematical function.

**Non-deterministic time series:** future values can be determined only in terms of a probability distribution.

---

## Stochastic processes

"A statistical phenomenon that evolves in time according to probabilistic laws is called a stochastic process." (Box, George EP, et al. Time series analysis: forecasting and control.)

## Non-deterministic time series or statistical time series

A sample realization from an infinite population of time series that could have been generated by a stochastic process.

---

## Types of methods

- Qualitative forecast

- Quantitative forecast

---
## Basic steps in a forecasting task

Problem definition

Collect data

Data visualization

Modelling

Evaluate the fitted model

---

## Frequency of a time series: Seasonal periods

![](1.png)

---

![](2.png)
---

# Your turn

 What are the frequencies for a monthly time series with semi-annual and annual pattern?

---

## Time series patterns

**Trend**

Long-term increase or decrease in the data.

**Seasonal**

A seasonal pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week). Seasonality is always of a fixed and known period. Hence, seasonal time series are sometimes called periodic time series.

Period is unchanging and associated with some aspect of the calendar.

---

**Cyclic**

- A cyclic pattern exists when data exhibit rises and falls that are not of fixed period. The duration of these fluctuations is usually of at least 2 years.
In general,

- the average length of cycles is longer than the length of a seasonal pattern.

- the magnitude of cycles tends to be more variable than the magnitude of seasonal patterns.

---

![](3.png)

---

![](4.png)

---

![](5.png)

---

![](6.png)

---

![](7.png)

---

![](8.png)

---

![](9.png)

---

![](10.png)

---

![](11.png)

---

![](12.png)

---

![](13.png)

---

![](14.png)

---

![](15.png)

---

![](16.png)

---

![](17.png)

---

![](18.png)

---

[Cont: click here](https://tsforecasting-thiyanga.netlify.app/slides/timeseries1#39)

[Basics of time series forecasting - 1](https://tsforecasting-thiyanga.netlify.app/slides/timeseries2#6)

[Basics of time series forecasting - 2](https://tsforecasting-thiyanga.netlify.app/slides/timeseriesforecasting3#4)
---

# Python

```{python}
#| fig-column: margin
#| echo: true
import numpy
import matplotlib.pyplot as plt

mean = 0
std = 1 
num_samples = 100
samples = numpy.random.normal(mean, std, size=num_samples)
plt.plot(samples)
plt.show()
```


# ACF 

```{python}
#| echo: true
import pandas as pd
from matplotlib import pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(samples, lags=20)
plt.show()
```

# Random walk

```{python}
#| echo: true
import numpy as np
rw = np.cumsum(samples)
plt.plot(rw)
plt.show()
```

# Random walk - ACF

```{python}
#| echo: true
plot_acf(rw, lags=20)
plt.show()
```

# Difference series

```{python}
#| echo: true
df = pd.DataFrame(rw, columns = ['Values'])
df['Lag 1'] = df['Values'].diff()
df['Lag 2'] = df['Values'].diff().diff()
df
```

# Plot Lag 1 series

```{python}
#| echo: true
plt.plot(df['Values'].diff())
plt.show()
```

# ACF Lag 1 series

```{python}
#| echo: true
diff = df['Lag 1']
plot_acf(diff.dropna(), lags=20)
plt.show()
```

# Example 2

```{python}
#| echo: true
#| eval: false
import numpy as np, pandas as pd
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt
plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})

# Import data
df = pd.read_csv('wwwusage.csv', names=['value'], header=0)

# Original Series
fig, axes = plt.subplots(2, 2, sharex=True)
axes[0, 0].plot(df.value); axes[0, 0].set_title('Original Series')
plot_acf(df.value, ax=axes[0, 1], lags=np.arange(len(df)))

# 1st Differencing
axes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('1st Order Differencing')
plot_acf(df.value.diff().dropna(), ax=axes[1, 1], lags=np.arange(len(df) - 1))
plt.show()

```

#

```{python}
#| echo: false
import numpy as np, pandas as pd
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt
plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})

# Import data
df = pd.read_csv('wwwusage.csv', names=['value'], header=0)

# Original Series
fig, axes = plt.subplots(2, 2, sharex=True)
axes[0, 0].plot(df.value); axes[0, 0].set_title('Original Series')
plot_acf(df.value, ax=axes[0, 1], lags=np.arange(len(df)))

# 1st Differencing
axes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('1st Order Differencing')
plot_acf(df.value.diff().dropna(), ax=axes[1, 1], lags=np.arange(len(df) - 1))
plt.show()


```

## 2nd order differencing

```{python}
#| echo: true
plot_acf(df.value.diff().diff().dropna())
plt.show()
```

## Monthly Airline Passenger Numbers 1949-1960

```{python}
#| echo: true
airpassenger = pd.read_csv('AirPassengers.csv')
from datetime import datetime
import plotnine
from plotnine import *
airpassenger['Month']= pd.to_datetime(airpassenger['Month'])
ggplot(airpassenger, aes(x='Month', y='#Passengers'))+geom_line()
```

## Monthly Airline Passenger Numbers 1949-1960 - log

```{python}
#| echo: true
import numpy as np
airpassenger['naturallog'] = np.log(airpassenger['#Passengers']) 
ggplot(airpassenger, aes(x='Month', y='naturallog'))+geom_line()
```

## Monthly Airline Passenger Numbers 1949-1960 - log

```{python}
#| echo: true
import numpy as np
airpassenger['naturallog'] = np.log(airpassenger['#Passengers']) 
ggplot(airpassenger, aes(x='Month', y='naturallog'))+geom_line()
```

## Box-Cox transformation

```{python}
#| echo: true
# import modules
import numpy as np
from scipy import stats
 
y2,fitted_lambda = stats.boxcox(airpassenger['#Passengers'])

```


##  Box-Cox transformation: Exploring the output

```{python}
#| echo: true

fitted_lambda
```

```{python}
#| echo: true
y2

```

# Step 1: Plot data

1. Detect unusual observations in the data

1. Detect non-stationarity by visual inspections of plots

Stationary series:

- has a constant mean value and fluctuates around the mean.

- constant variance.

- no pattern predictable in the long-term.

##

```{python}
from sktime import *
from sktime.datasets import load_airline
from sktime.utils.plotting import plot_series
y = load_airline()
plot_series(y)
```

## Step 2: Split time series into training and test

Specify the forecast horizon

```{python}
#| echo: true
import numpy as np
import pandas as pd
from sktime.forecasting.base import ForecastingHorizon
fh = ForecastingHorizon(
    pd.PeriodIndex(pd.date_range("1960-01", periods=12, freq="M")), is_relative=False
)
fh

```

## Plot training and test series

```{python}
from sktime.forecasting.model_selection import temporal_train_test_split
y_train, y_test = temporal_train_test_split(y, fh=fh)
plot_series(y_train, y_test, labels=["y_train", "y_test"])
```

##

1. Need transformations?

2. Need differencing?

## Step 3: Apply transformations

```{python}
#| echo: true
import numpy as np
y_train.naturallog = np.log(y_train) 
plot_series(y_train.naturallog)
```

## Step 4: Take difference series

**Identifying non-stationarity by looking at plots**
  
- Time series plot

- The ACF of stationary data drops to zero relatively quickly.

- The ACF of non-stationary data decreases slowly.

- For non-stationary data, the value of $r_1$ is often large and positive.

## Non-seasonal differencing and seasonal differencing

**Non seasonal first-order differencing:** $Y'_t=Y_t - Y_{t-1}$

<!--Miss one observation-->

**Non seasonal second-order differencing:** $Y''_t=Y'_t - Y'_{t-1}$

<!--Miss two observations-->

**Seasonal differencing:** $Y_t - Y_{t-m}$

<!--To get rid from prominent seasonal components. -->

- For monthly, $m=12$, for quarterly, $m=4$.

<!--We will loosefirst 12 observations-->


- Seasonally differenced series will have $T-m$ observations.
<!--Usually we do not consider differencing more than twice. -->

> There are times differencing once is not enough. However, in practice,it is almost never necessary to go beyond second-order differencing.
<!--Even the second-order differencing is very rare.-->

## ACF of log-transformation series

```{python}
#| echo: true
plot_acf(y_train.naturallog, lags=50)
```

## Take seasonal difference series

```{python}
#| echo: true
y_train.naturallog.diff12 = y_train.naturallog.diff(12)
y_train.naturallog.diff12
```

## Take seasonal difference series (cont.)

```{python}
#| echo: true
y_train.naturallog.diff12.head(20)
```



## ACF - diff(log(data), 12)

```{python}
#| echo: true
plot_acf(y_train.naturallog.diff12.dropna(), lags=50)
plt.show()
```

## ACF - First differencing on diff(log(data), 12)

```{python}
#| echo: true
y_train.naturallog.diff12.diff = y_train.naturallog.diff12.diff()
plot_acf(y_train.naturallog.diff12.diff.dropna(), lags=50)
plt.show()
```

## PACF - First differencing on diff(log(data), 12)


```{python}
#| echo: true
plot_pacf(y_train.naturallog.diff12.diff.dropna(), lags=50)
plt.show()
```

## Testing for nonstationarity for the presence of unit roots

- Dickey and Fuller (DF) test

- Augmented DF test

- Phillips and Perron (PP) nonparametric test

- Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test

## KPSS test
H0: Series is level or trend stationary.

H1: Series is not stationary.

## KPSS test

```{python}
#| echo: true
from statsmodels.tsa.stattools import kpss
def kpss_test(series, **kw):    
    statistic, p_value, n_lags, critical_values = kpss(series, **kw)
    # Format Output
    print(f'KPSS Statistic: {statistic}')
    print(f'p-value: {p_value}')
    print(f'num lags: {n_lags}')
    print('Critial Values:')
    for key, value in critical_values.items():
        print(f'   {key} : {value}')
    print(f'Result: The series is {"not " if p_value < 0.05 else ""}stationary')

kpss_test(y_train.naturallog)

```

## KPSS test

```{python}
#| echo: true
kpss_test(y_train.naturallog.diff12.dropna())
```

```{python}
#| echo: true
kpss_test(y_train.naturallog.diff12.diff.dropna())
```

## KPSS test

- KPSS test may not necessarily reject the null hypothesis (that the series is level or trend stationary) even if a series is steadily increasing or decreasing.

- The word ‘deterministic’ implies the slope of the trend in the series does not change permanently. That is, even if the series goes through a shock, it tends to regain its original path.

source: https://www.machinelearningplus.com/time-series/kpss-test-for-stationarity/

## KPSS test


- By default, it tests for stationarity around a ‘mean’ only.

- To turn ON the stationarity testing around a trend, you need to explicitly pass the regression='ct' parameter to the kpss

```{python}
#| echo: true
kpss_test(y_train.naturallog.diff12.dropna(), regression='ct')
```

```{python}
#| echo: true
kpss_test(y_train.naturallog.diff12.diff.dropna())
```

## ADF test

```{python}
#| echo: true
from statsmodels.tsa.stattools import adfuller

def adf_test(series):
    result = adfuller(series, autolag='AIC')
    print(f'ADF Statistic: {result[0]}')
    print(f'p-value: {result[1]}')
    for key, value in result[4].items():
        print('Critial Values:')
        print(f'   {key}, {value}')

series = df.loc[:, 'value'].values


```

H0: Series is not stationary

H1: Series is stationary

## ADF test

```{python}
#| echo: true
adf_test(y_train.naturallog)
```

```{python}
#| echo: true
adf_test(y_train.naturallog.diff12.dropna())
```

```{python}
#| echo: true
adf_test(y_train.naturallog.diff12.diff.dropna())
```

## KPSS vs ADF test

If a series is stationary according to the KPSS test by setting regression='ct' and is not stationary according to the ADF test, it means the series is stationary around a deterministic trend.

Further reading: 

Kwiatkowski, D.; Phillips, P. C. B.; Schmidt, P.; Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root. Journal of Econometrics, 54 (1-3): 159-178.

# Step 5: Examine the ACF/PACF to identify a suitable model

## AR(p)

- ACF dies out in an exponential or damped
sine-wave manner.

- there is a significant spike at lag $p$ in PACF, but
none beyond $p$.

## MA(q)

- ACF has all zero spikes beyond the $q^{th}$ spike.

- PACF dies out in an exponential or damped
sine-wave manner.

## Seasonal components

- The seasonal part of an AR or MA model will be seen
in the seasonal lags of the PACF and ACF.


## ARIMA(0,0,0)(0,0,1)12 will show
 
  - a spike at lag 12 in the ACF but no other significant spikes.

  - The PACF will show exponential decay in the seasonal lags  12, 24, 36, . . . .
  
## ARIMA(0,0,0)(1,0,0)12 will show

  - exponential decay in the seasonal lags of the ACF.
    
  - a single significant spike at lag 12 in the PACF.


## Step 5: Examine the ACF/PACF to identify a suitable model (cont.)

- $d=1$ and $D=1$ (from step 4)

- Significant spike at lag 1 in ACF suggests
non-seasonal MA(1) component.

- Significant spike at lag 12 in ACF suggests seasonal
MA(1) component.

- Initial candidate model: $ARIMA(0,1,1)(0,1,1)_{12}$.

- By analogous logic applied to the PACF, we could also have started with $ARIMA(1,1,0)(1,1,0)_{12}$.
  
## Models   
  
**Initial model:**

$ARIMA(0,1,1)(0,1,1)_{12}$

$ARIMA(1,1,0)(1,1,0)_{12}$

**Try some variations of the initial model:**

$ARIMA(0,1,1)(1,1,1)_{12}$

$ARIMA(1,1,1)(1,1,0)_{12}$

$ARIMA(1,1,1)(1,1,1)_{12}$

##

**Try some variations**

Both the ACF and PACF show significant spikes at lag 3, and almost significant spikes at lag 3, indicating that some additional non-seasonal terms need to be included in the model.

$ARIMA(3,1,1)(1,1,1)_{12}$

$ARIMA(1,1,3)(1,1,1)_{12}$

$ARIMA(3,1,3)(1,1,1)_{12}$

## Fitting ARIMA models

```{python}
#| echo: true
from sktime.forecasting.arima import ARIMA
forecaster1 = ARIMA(  
    order=(1, 1, 0),
    seasonal_order=(1, 1, 0, 12),
    suppress_warnings=True)
forecaster1.fit(y_train.naturallog)    
```


## Step 6: Check residual series

```{python}
#| echo: true
fhtrain = ForecastingHorizon(
    pd.PeriodIndex(pd.period_range(start='1949-01', end='1959-12', freq='M')), is_relative=False
)
fhtrain

```

## Obtain predictions for the training period

```{python}
#| echo: true
y_pred_train = forecaster1.predict(fhtrain)
y_pred_train

```

## Obtain residual series

```{python}
#| echo: true
residual = y_train.naturallog - y_pred_train
residual
```

## Plot residuals

```{python}
#| echo: true
plot_series(residual)
```

## Plot residuals (cont.)

```{python}
#| echo: true
plot_acf(residual.dropna(), lags=50)
```

## Plot residuals (cont.)

```{python}
#| echo: true
import matplotlib.pyplot as plt
import numpy as np
plt.hist(residual)
plt.show()
```

Your turn: remove the outlier and draw the histogram

## Ljung-Box Test

H0: Residuals are not serially correlated.

H1: Residuals are serially correlated.

```{python}
import statsmodels.api as sm
sm.stats.acorr_ljungbox(residual.dropna(), lags=[20], return_df=True)
```


## Step 7: Generate forecasts


```{python}
#| echo: true
y_pred_1 = forecaster1.predict(fh)
y_pred_1

```

## Back transformation

```{python}
#| echo: true
y_pred_1.exp = np.exp(y_pred_1)
y_pred_1.exp
```

## Plot training, test, and forecasts

```{python}
#| echo: true
plot_series(y_train, y_test, y_pred_1.exp, labels=["y_train", "y_test", "y_forecast"])

```

## Evaluation

```{python}
#| echo: true
from sktime.performance_metrics.forecasting import \
    mean_absolute_percentage_error
mean_absolute_percentage_error(y_test, y_pred_1.exp, symmetric=False)
```

## Your Turn

Fit other variants of ARIMA models and identify the best ARIMA model for the series.

## Modelling steps

1. Plot the data.

2. If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.

3. If the data are non-stationary, take first differences of the data until the data are stationary.

4. Examine the ACF/PACF to identify a suitable model.

5. Try your chosen model(s), and use the AICc to search for a better model.

## Modelling steps (cont.)

6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.

7. Once the residuals look like white noise, calculate forecasts.



Source: Forecasting: Principles and Practice, Rob J Hyndman and George Athanasopoulos

## Forecasting with Python

[Lab](https://github.com/thiyangt/spatiotemporal_lab/blob/main/7_timeseries_sktime.ipynb)